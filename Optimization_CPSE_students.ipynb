{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization_CPSE_students.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyON7zZwsx6tlbw1VO43evoL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edgarsmdn/Optimization_CPSE_2020/blob/master/Optimization_CPSE_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlT5RZ2KxQ5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authors:\n",
        "## Antonio del Rio Chanona      https://www.imperial.ac.uk/people/a.del-rio-chanona\n",
        "## Edgar Ivan Sanchez Medina    https://www.mpi-magdeburg.mpg.de/person/103552/2316\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from matplotlib.animation import FuncAnimation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTgaIpQpzJXO",
        "colab_type": "text"
      },
      "source": [
        "Along this notebook you will find <font color='blue'>text in blue that describe the **coding tasks** that you have to implement.</font> Within the code cell, your implementation needs to be between the line:\n",
        "\n",
        "`#-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#`\n",
        "\n",
        "and the line:\n",
        "\n",
        "`#-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#-#`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HznFdIzf0rKR",
        "colab_type": "text"
      },
      "source": [
        "# **1. Optimization basics**\n",
        "\n",
        "An optimization problem has the form\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x)\\\\\n",
        "\\textrm{s.t.} \\quad & g_i(x) \\leq 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The vector $x = (x_1,..., x_n)$ is the optimization variable of the problem, the function $f : \\mathbb{R}^n \\to \\mathbb{R} $ is the objective function, the functions $g_i : \\mathbb{R}^n \\to \\mathbb{R} $,\n",
        "$i = 1, ...,m$, are the (inequality) constraint functions. A vector $x^â‹†$ is called optimal, if it has the smallest objective value among all vectors\n",
        "that satisfy the constraints.\n",
        "\n",
        "**Convex optimization**\n",
        "\n",
        "A convex optimization problem is one in which the objective and the\n",
        "constraint functions are convex, which means they satisfy the inequality:\n",
        "\n",
        "\\begin{equation}\n",
        "f(\\alpha x + \\beta y) \\leq \\alpha f(x) + \\beta f(y)\n",
        "\\end{equation}\n",
        "\n",
        "for all $x,y \\in \\mathbb{R}^n$ and all $\\alpha, \\beta \\in \\mathbb{R}$ with $\\alpha + \\beta = 1$, $\\alpha  \\geq 0$ and $\\beta  \\geq 0$\n",
        "\n",
        "It is worth nothing that any equality constraint can be represented as two inequality constraints. This enforce that, in a convex problem, the equality constraints must be linear.\n",
        "\n",
        "Therefore, an optimization problem is convex whenever the following requirements are met:\n",
        "\n",
        "* The objective function is convex.\n",
        "* The inequality constraints are convex.\n",
        "* The equality constraints are linear.\n",
        "\n",
        "**Global and local optima**\n",
        "\n",
        "An optimal solution $x^*$ is said to be the global optimum when the constraints are met at this point and $f(x^*) \\leq f(x),  \\forall x \\in X$. When this condition is met only whitin a certain neightborhood, the solution is called local optimum.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkbFE6Kd091N",
        "colab_type": "text"
      },
      "source": [
        "# **2. Gradient descent**\n",
        "\n",
        "The optimization methods known as \"descent methods\" minimize a function by applying the following update rule at each iteration:   \n",
        "\n",
        "\\begin{equation}\n",
        "x^{(k+1)} = x^{(k)} +  \\alpha^{(k)} \\Delta x^{(k)}\n",
        "\\end{equation}\n",
        "\n",
        "in this rule, $\\Delta^{(k)}$ denotes the **direction** at the iteration $k$, and $\\alpha^{(k)} \\geq 0$ is a scalar value called **step size**. These methods are called descent methods, because at each iteration $f \\left(x^{(k+1)} \\right) \\leq  f \\left(x^{(k)} \\right)$.\n",
        "\n",
        "In the method called *Gradient Descent* the direction is chosen to be the negative of the gradient: $ \\Delta x := - \\nabla f(x) $. therefore, the algorithm is:\n",
        "\n",
        "**Algorithm**\n",
        "\n",
        "1. Given a starting point $x \\in X$.\n",
        "2. Repeat\n",
        "3. $~~~~~~$ $ \\Delta x := - \\nabla f(x) $\n",
        "2. $~~~~~~$ Choose $\\alpha$.\n",
        "3. $~~~~~~$ Update: $ x = x +  \\alpha \\Delta x$\n",
        "4. until stopping criterion is met.\n",
        "\n",
        "<font color='blue'>Code step 3 and 5 within the following code.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enTMXZt2xi-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################\n",
        "# --- Gradient Descent --- #\n",
        "############################\n",
        "\n",
        "def gradient_descent(f, x0, grad_f, lr, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Gradient Descent\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        lr       : Learning rate\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "        grad_i  =      # compute gradient\n",
        "        x       =      # compute step    \n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Gradient Descent \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOW1KIy7R2Wr",
        "colab_type": "text"
      },
      "source": [
        "One way to approximate the (numerically) gradient of a function  is the **finite diffirences method**. There exist mainly three type of finite difference approximations:\n",
        "\n",
        "* Backward difference  $f'(x) \\approx \\frac{f(x_k) - f(x_k - \\epsilon)}{\\epsilon}$\n",
        "* Forward difference  $f'(x) \\approx \\frac{f(x_k + \\epsilon) + f(x_k)}{\\epsilon}$\n",
        "* Central difference $f'(x) \\approx \\frac{f(x_k + \\frac{\\epsilon}{2}) - f(x_k - \\frac{\\epsilon}{2})}{\\epsilon}$\n",
        "\n",
        "However, the central difference approximation gives the most accurate one among these three. Therefore, let's implement that one here.\n",
        "\n",
        "<font color='blue'>Implement the **central finite differences**.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMycAmM0SOji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################    \n",
        "# --- Central finite differences --- #\n",
        "######################################\n",
        "\n",
        "def central_finite_diff(f, x):\n",
        "      '''\n",
        "      Central finite differences approximation.\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the gradient\n",
        "      OUTPUTS:\n",
        "          grad: Approximation of the gradient of f at x \n",
        "      '''\n",
        "      dim = x.shape[0]\n",
        "      eps  = np.sqrt(np.finfo(float).eps)  # Step-size is the square root of the machine precision\n",
        "      grad = np.zeros((1,dim))\n",
        "      \n",
        "      for i in range(dim):\n",
        "          e           = np.zeros((1,dim))\n",
        "          e[0,i]      = eps\n",
        "          #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "          grad_approx =       # compute central finite diff.\n",
        "          #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#-#\n",
        "          grad[0,i]     = grad_approx\n",
        "      \n",
        "      return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6p9RFf9EsWj",
        "colab_type": "text"
      },
      "source": [
        "The next cell contains the test function that we are going to use here, but this can be replace by any function. In order to approximate the gradients we are going to use the central finite differences method with five-points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTes4LZfzw96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Rosenbrock_f(x):\n",
        "    '''\n",
        "    Rosenbrock function\n",
        "    '''\n",
        "    n = np.shape(x)[1]\n",
        "    z = np.sum(100*(x[:,1:] - x[:,:n-1]**2)**2 + (x[:,:n-1] - 1)**2, axis=1)\n",
        "    return z\n",
        "\n",
        "###############################################\n",
        "# --- Central finite differences 5 points --- # \n",
        "###############################################\n",
        "\n",
        "def central_finite_diff5(f, x):\n",
        "      '''\n",
        "      Five-points method for central finite differences.\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the gradient\n",
        "      OUTPUTS:\n",
        "          grad: Approximation of the gradient of f at x \n",
        "      '''\n",
        "      dim = x.shape[1]\n",
        "      # Step-size is taken as the square root of the machine precision\n",
        "      eps  = np.sqrt(np.finfo(float).eps) \n",
        "      grad = np.zeros((1,dim))\n",
        "        \n",
        "      for i in range(dim):\n",
        "          e           = np.zeros((1,dim))\n",
        "          e[0,i]      = eps\n",
        "          grad_approx = (f(x - 2*e) - 8*f(x - e) + 8*f(x + e) - f(x + 2*e) )/(12*eps) \n",
        "          \n",
        "          grad[0,i]     = grad_approx\n",
        "        \n",
        "      return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dlj9-ndyPhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Gradient Descent --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = gradient_descent(Rosenbrock_f, x0, central_finite_diff5, 0.001, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-sSAU1qEBqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot function\n",
        "x_1 = np.linspace(0,1)\n",
        "x_2 = np.linspace(0,1)\n",
        "X, Y = np.meshgrid(x_1, x_2)\n",
        "Z = Rosenbrock_f(np.append(X.reshape(-1,1), Y.reshape(-1,1), axis=1))\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "x_list    = np.array(x_list).reshape(-1,x0.shape[1])\n",
        "\n",
        "x_summary = []\n",
        "f_summary = []\n",
        "for i in range(x_list.shape[0]):\n",
        "  if i % 100 == 0:\n",
        "    x_summary.append(x_list[i])\n",
        "    f_summary.append(f_list[i])\n",
        "x_summary = np.array(x_summary).reshape(-1,x0.shape[1])\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "display_value = ax.text(0.05, 0.2, '', transform=ax.transAxes)\n",
        "plt.close()\n",
        "\n",
        "def animate(i):\n",
        "    ax.plot(x_summary[:i, 0], x_summary[:i, 1], 'k.', alpha=0.6)    # Animate points\n",
        "    display_value.set_text('Min = ' + str(f_summary[i]))          # Animate display value\n",
        "    ax.set_title('Rosenbrock function, Iteration: ' + str(i*100))  # Animate title\n",
        "    return display_value\n",
        "\n",
        "anim = FuncAnimation(fig, animate, frames=len(f_summary), interval=100, repeat_delay=800)\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1THzQFSFQFX",
        "colab_type": "text"
      },
      "source": [
        "# **3. Steepest descent**\n",
        "\n",
        "By contrast, the steepest descent method uses the steepest descent direction (as opposed to the negative gradient in Gradient Descent) as the direction to move to at each iteration.\n",
        "\n",
        "In order to find the steepest direction, we can approximate\n",
        "the function via a first-order Taylor expansion:\n",
        "\n",
        "\\begin{equation}\n",
        "f(x + \\Delta x) \\approx f(x) + \\nabla f(x)^T \\Delta x\n",
        "\\end{equation}\n",
        "\n",
        "The second term of this approximation, $\\nabla f(x)^T \\Delta x$,  gives the approximate change in $f$ for a small step $\\Delta x$. Therefore, to minimize the function $f$ we have to make this second term, $\\nabla f(x)^T \\Delta x$, as negative as possible. To do this, we have to choose the step $\\Delta x$ such that $\\Delta x = \\text{argmin} \\{ \\nabla f(x)^T \\Delta x | \\| \\Delta x \\| =1 \\}$. The constraint, $\\| \\Delta x \\| = 1$, ensures that the solution to this problem is sensible, othewise a very large $\\Delta x$ would always be chosen. \n",
        "\n",
        "**Algorithm**\n",
        "\n",
        "1. Given a starting point $x \\in X$.\n",
        "2. Repeat\n",
        "3. $~~~~~~$ $ \\Delta x := \\|f(x)\\| \\ast \\text{argmin} \\{ \\nabla f(x)^T \\Delta x | \\| \\Delta x \\| =1 \\} $\n",
        "2. $~~~~~~$ Choose $\\alpha$.\n",
        "3. $~~~~~~$ Update: $ x = x +  \\alpha \\Delta x$\n",
        "4. until stopping criterion is met.\n",
        "\n",
        "In general, we may consider various norms for the minimization problem. And the interpretation of steepest descent with different norms leads to different algorithms. For example, by using the $l_2$ norm:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta x = \\text{argmin} \\{ \\nabla f(x)^T \\Delta x | \\| \\Delta x \\|_2 =1 \\}\n",
        "\\end{equation}\n",
        "\n",
        "From the Cauchy-Schwarz inequality we know that \n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla f(x)^T \\Delta x \\leq \\| \\nabla f(x) \\| \\| \\Delta x \\| \n",
        "\\end{equation}\n",
        "\n",
        "with equality when $\\Delta x = \\lambda \\nabla f(x)$, $\\lambda \\in \\mathbb{R}$. Since $ \\| \\Delta x \\| = 1$ and we want to minimize:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta x^* = - \\frac{\\nabla f(x)}{\\| \\nabla f(x) \\|}\n",
        "\\end{equation}\n",
        "\n",
        "and the update rule becomes exactly equal to the Gradient Descent one (this is not the case if we take a different norm). For this reason we say that Gradient Descent is just a special case of Steepest Descent.\n",
        "\n",
        "#### **Exact line search**\n",
        "\n",
        "Ideally we would need to take the step size $\\alpha$ (aka learning rate in the Machine Learning field) that minimizes the funtion at the next point. In other words we would need to solve the following in order to get the optimal step size at each iteration:\n",
        "\n",
        "\\begin{equation}\n",
        "\\alpha^* \\in \\text{argmin}_{\\alpha \\geq 0} ~~~ f \\left( x^{(k)} - \\alpha \\nabla f \\left(x^{(k)} \\right) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "Solving the above problem is known as *exact line search*, but this would require to solve an adittional optimization problem that can be computationaly expensive in most applications. Therefore, the learning rate $\\alpha$ is most of the times chosen heuristically or using other approximations to the exact line search (e.g. backtracking).\n",
        "\n",
        "#### **Backtracking line search**\n",
        "\n",
        "As mentioned before, backtracking line search is one of the approximation methods for the exact line search, and it relies on two constants $A$ and $B$, such that $0 < A < 0.5$ and $0 < B < 1$. The role of the constant $A$ is to reduce the slope of the line in which the search will be performed (see following plot), and the constant $B$ weights the previous learning rate $\\alpha$.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1foXJkVjqcSmP4GN3nmhEdHFUBP48DYCP)\n",
        "\n",
        "\n",
        "Therefore, at each iteration we would have:\n",
        "\n",
        "* Set $\\alpha = 1$\n",
        "* While the following condition holds: $f \\left( x - \\alpha \\nabla f(x) \\right) > f(x) - A \\alpha \\nabla f(x)^T \\nabla f(x)$, reduce $\\alpha$ according to the following: $\\alpha := B \\alpha$. The inequality condition used here is known as the Armijoâ€“Goldstein condition.\n",
        "\n",
        "Typical values for these two constants are $A = [0.01, 0.3]$ and $B = [0.1, 0.8]$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOrePufC1LU3",
        "colab_type": "text"
      },
      "source": [
        "# **4. Gradient descent with momentum**\n",
        "\n",
        "The idea behind this extension is to avoid most of the (unnecesary) zig-zag movements of Gradient descent by accumulating momentum along the direction towards the optimum while we iterate. \n",
        "\n",
        "Therefore, the update rule of Gradient Descent is modified like this:\n",
        "\n",
        "\\begin{equation}\n",
        "x^{(k+1)} = x^{(k)} + v^{(k)}\n",
        "\\end{equation}\n",
        "\n",
        "where $v^{(k)}$ is the velocity term defined by:\n",
        "\n",
        "\\begin{equation}\n",
        "v^{(k)} = \\beta v^{(k-1)} - \\alpha \\nabla f(x^{(k)}) \n",
        "\\end{equation}\n",
        "\n",
        "where $\\beta \\in [0,1]$ is the momentum hyperparameter commonly set to 0.9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxCQhvBuycUd",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Code a function for the **velocity** term calculation and a function for the **line search** method. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G0TK28byqyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# --- Momentum --- #\n",
        "####################\n",
        "\n",
        "def momentum(grad_i, v_prev, lr, beta=0.9):\n",
        "    '''\n",
        "    Momentum function\n",
        "    INPUTS:\n",
        "        grad_i  : Gradient of function at current position\n",
        "        v_prev  : velocity value at the previous position\n",
        "        beta    : Momentum hyperparameter\n",
        "    OUTPUTS:\n",
        "        v       : Velocity term\n",
        "    '''\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "    v =               # compute velocity term\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "    return v\n",
        "\n",
        "#######################\n",
        "# --- Line search --- #\n",
        "#######################\n",
        "\n",
        "def ls(grad_i, x, f):\n",
        "    '''\n",
        "    Line search for determining learning rate\n",
        "    INPUTS:\n",
        "        grad_i  : Gradient of function at current position\n",
        "        x       : Current position\n",
        "        f       : Objective function\n",
        "    OUTPUTS:\n",
        "        lr    : Optimal learning rate\n",
        "        iter  : Number of iterations needed in line search\n",
        "    '''\n",
        "    iter = 0\n",
        "    lr   = 1\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "          # compute line search loop\n",
        "                 \n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "        iter += 1\n",
        "    \n",
        "    return lr, iter\n",
        "\n",
        "\n",
        "#############################################\n",
        "# --- Line search with Armijo-Goldstein --- #\n",
        "#############################################\n",
        "\n",
        "def line_search(grad_i, x, f, A=0.1, B=0.8):\n",
        "    '''\n",
        "    Line search for determining learning rate\n",
        "    INPUTS:\n",
        "        grad_i  : Gradient of function at current position\n",
        "        x       : Current position\n",
        "        f       : Objective function\n",
        "    OUTPUTS:\n",
        "        lr    : Optimal learning rate\n",
        "        iter  : Number of iterations needed in line search\n",
        "    '''\n",
        "    iter = 0\n",
        "    lr   = 1\n",
        "    # Armijo-Goldstein condition loop\n",
        "    while f(x - lr*grad_i) > f(x) - A*lr*np.dot(grad_i,grad_i.T) and iter<100:\n",
        "        lr  = B*lr          \n",
        "        iter += 1\n",
        "    \n",
        "    return lr, iter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMfT7e6lyxqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################################\n",
        "# --- Gradient Descent with line search and momentum --- #\n",
        "##########################################################\n",
        "\n",
        "def GD_ls_momentum(f, x0, grad_f, beta=0.9, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Gradient Descent with line search and momentum\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta     : Parameter beta for the momentum calculation\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    v_prev = 0      # initialize at zero to get normal GD at first step\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                               # compute gradient\n",
        "        lr      = line_search(grad_i, x, f)[0]              # compute learning rate using line search\n",
        "        v       = momentum(grad_i, v_prev, lr, beta=beta)   # compute momentum\n",
        "        x       = x + v                                     # compute step \n",
        "        v_prev  = v                                         # update previous momentum term   \n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Gradient Descent with momentum \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScRGyx0tzx-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Gradient Descent with line search and momentum --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = GD_ls_momentum(Rosenbrock_f, x0, central_finite_diff5, beta=0.95, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYNQeRLN0ZUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with line search and momentum')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuCBZ6Hb12l5",
        "colab_type": "text"
      },
      "source": [
        "# **5. Nesterov Accelerated Gradient Descent**\n",
        "\n",
        "The Nesterov Accelerated Gradient Descent (NAG) is a further improvement to the Gradient Descent with momentum algorithm. The step direction in NAG is calculated based on the gradient on an approximated future position instead of the current position, in this way, more gradient information is included into the update step compared to the traditional momentum approach.\n",
        "\n",
        "Therefore, the velocity term in NAG is determined by:\n",
        "\n",
        "$v^{(k)} = \\beta v^{(k-1)} - \\alpha \\nabla f(\\tilde{x}^{(k)})$\n",
        "\n",
        "where $\\tilde{x}^{(k)}$ is the approximated future position calculated as:\n",
        "\n",
        "$\\tilde{x}^{(k)} = x^{(k)} + \\beta v^{(k-1)}$\n",
        "\n",
        "<font color='blue'>Code a function for the **nesterov** calculation of velocity. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbrQjOmLGfi1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# --- Nesterov --- #\n",
        "####################\n",
        "\n",
        "def nesterov(grad_tilde, v_prev, lr, beta=0.9):\n",
        "    '''\n",
        "    Momentum function\n",
        "    INPUTS:\n",
        "        grad_tilde  : Gradient of function at nesterov modified position\n",
        "        v_prev      : velocity value at the previous position\n",
        "        beta        : Momentum hyperparameter\n",
        "    OUTPUTS:\n",
        "        v           : Velocity term\n",
        "    '''\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-# \n",
        "    v =               # compute the Nesterov velocity term\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTHkgNmOGrpP",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Implement your function **`nesterov` into the `NAG`** function bellow</font>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC6n6HwHGjHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#################################\n",
        "# --- NAG with line search  --- #\n",
        "#################################\n",
        "\n",
        "def NAG(f, x0, grad_f, beta=0.9, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Nesterov Accelerated Gradient Descent with line search\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta     : Parameter beta for the momentum calculation\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    v_prev = 0      # initialize at zero to get normal GD-momentum at first step\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                               # compute gradient at current position\n",
        "        \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-# \n",
        "        x_tilde =             # nesterov modified position\n",
        "        g_tilde =             # compute gradient of function at x_tilde\n",
        "        lr      =             # compute learning rate using line search\n",
        "        v       =             # compute momentum\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-# \n",
        "\n",
        "        x       = x + v                                     # compute step \n",
        "        v_prev  = v                                         # update previous momentum term   \n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using NAG \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxD3RqIgG5Go",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- NAG --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = NAG(Rosenbrock_f, x0, central_finite_diff5, beta=0.95, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2hRk4oFHFDx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with NAG')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_vnuTOQ2KrY",
        "colab_type": "text"
      },
      "source": [
        "# **6. AdaGrad**\n",
        "\n",
        "We have been using line search as a way to determine a reasonable value for the learning rate. However, in such approach, the learning rate $\\alpha$ is a unique value that is applied in all dimensions of the gradient. Can you see the problem of this? Imagine you have different ranges on each dimension of the gradient, then the learning rate will favour each dimension differently. One straightforward approach would be to have a learning rate for each dimension, i.e. the $\\alpha$ would be a vector instead of a scalar. The problem with this is that in many applications you can easily end-up with thousand or millions of dimensions (think about a Deep Neural Network). Therefore, one of the first approaches to this problem was AdaGrad, which adaptively scale the learning rate for each dimension. Another advantage is that we do not have to adjust the learning rate manually, as this is adaptively adjusted.\n",
        "\n",
        "The way in which AdaGrad does what we explained above is to update the position according to:\n",
        "\n",
        "$x^{(k)} = x^{(k)} - \\alpha \\frac{\\nabla f(x^{(k)})}{V^{(k)}}$\n",
        "\n",
        "where $V^{(k)}$ is the accumulate historical gradient that is calculated as:\n",
        "\n",
        "$V^{(k)} = \\sqrt{\\sum_{i=1}^{k} \\left(\\nabla f(x^{(k)})\\right)^2 + \\epsilon}$\n",
        "\n",
        "where $\\epsilon$ is just a small number that prevents the division by zero.\n",
        "\n",
        "<font color='blue'>Code a function for the **gradient historical accumulation** term. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdJXvLVZJjM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################\n",
        "# --- Gradient historical accumulation --- #\n",
        "############################################\n",
        "\n",
        "def grad_accumulation(g_traj, eps=1e-8):\n",
        "    '''\n",
        "    Gradient historical accumulation\n",
        "    Note: A vectorized form might be more efficient, but we will keep it like \n",
        "          this, for educational purposes.\n",
        "    INPUTS:\n",
        "        g_traj  : List of historical gradients\n",
        "        eps     : small number to prevent division by zero\n",
        "    OUTPUTS:\n",
        "        V       : Gradient accumulation\n",
        "    '''\n",
        "    dim    = g_traj[0].shape[1]\n",
        "    g_traj = np.array(g_traj).reshape(-1, dim)\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-# \n",
        "    V      =          # compute accumulation historical gradient\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "    return V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTmJSALmJrA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################\n",
        "# --- AdaGrad --- #\n",
        "###################\n",
        "\n",
        "def adagrad(f, x0, grad_f, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    AdaGrad optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    g_traj = []             # To store gradient trajectory\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                  # compute gradient\n",
        "        g_traj.append(grad_i)                  # save gradient\n",
        "        lr      = 0.01                        # learning rate\n",
        "        if iter_i == 0:\n",
        "          x  = x - lr*grad_i                # compute step at first iteration\n",
        "        else:\n",
        "          V  = grad_accumulation(g_traj)\n",
        "          x  = x - lr*grad_i/V              # compute step\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "\n",
        "        # print optimization status\n",
        "        if iter_i % 2000 == 0:\n",
        "          print('Current iteration: ', iter_i)\n",
        "        \n",
        "    print(' Optimization using AdaGrad \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XD4xN9tJtGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- AdaGrad --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = adagrad(Rosenbrock_f, x0, central_finite_diff5, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ll30S-hKgSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with AdaGrad')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izgX-vVRKiPk",
        "colab_type": "text"
      },
      "source": [
        "As you can see, the problem becomes evident: **for this problem AdaGrad is super slow!**\n",
        "This is because, of course, by reducing the gradient at each iteration the steps that we take are very small. And, since we are accumulating the gradients, we are reducing the gradient by a larger amount at each iteration. Therefore, the nice feature of AdaGrad of taking care of the different ranges in the dimensions of the gradient comes at the expense of very slow convergence.\n",
        "\n",
        "This takes us to an important point in optimization: **Always scale your features!**\n",
        "\n",
        "You can use [standarization or min-max sclaing](https://en.wikipedia.org/wiki/Feature_scaling) for example, but the idea of scaling your features is that you overcome this difficulty of having enormous differences in the ranges in your gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr0Dt0vD2SpS",
        "colab_type": "text"
      },
      "source": [
        "# **7. AdaDelta**\n",
        "\n",
        "AdaDelta is an extension of AdaGrad that tries to overcome the slow convergence by not considering all the accumulated historical gradients, but only the gradients in a certain window over a given period. This is accomplished by calculating $V^{(k)}$ as follows:\n",
        "\n",
        "$V^{(k)} = \\sqrt{\\rho V^{(k-1)} + (1-\\rho) \\left(\\nabla f(x^{(k)})\\right)^2 + \\epsilon}$\n",
        "\n",
        "where $\\rho$ is a exponential decay parameter, usually set to 0.95. The reason for this specific form is that, instead of inneficiently store $w$ number of gradients, the same calculation can be obtained as the exponentially decaying average of the squared gradients that we see in the above expression.\n",
        "\n",
        "In the [original paper](https://arxiv.org/pdf/1212.5701.pdf), the authors noted that the hypothetical units in the update are not consistent. Hence, they defined an accumulation term also for the parameters updates. For this reason, they also definied the accumulation of the parameter updates as follows:\n",
        "\n",
        "$E^{(k)} = \\sqrt{\\rho E^{(k-1)} + (1-\\rho) \\Delta {x^{(k)}}^2 + \\epsilon}$\n",
        "\n",
        "and since  $\\Delta x^{(k)}$ is unknown at the current iteration, it is approximated using the previous position as:\n",
        "\n",
        "$ \\Delta x^{(k)} = - \\frac{E^{(k-1)}}{V^{(k)}} \\nabla f(x^{(k)}) $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHYov7fELQiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# --- AdaDelta --- #\n",
        "####################\n",
        "\n",
        "def adadelta(f, x0, grad_f, rho=0.95, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    AdaDelta optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        rho      : Exponential decay parameter\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    E_g = 0; E_x = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "        E_g     = rho*E_g + (1-rho)*grad_i*grad_i                       # exponential decay average on gradients\n",
        "        x_delta = - np.sqrt(E_x + eps)*grad_i/np.sqrt(E_g + eps)        # compute x_delta\n",
        "        E_x     = rho*E_x + (1-rho)*x_delta*x_delta                     # exponential decay average on parameters\n",
        "        x       = x + x_delta                                           # compute step\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using AdaDelta \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxE50Yb4LVof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- AdaDelta --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = adadelta(Rosenbrock_f, x0, central_finite_diff5, rho=0.9, eps=1e-8, max_iter=3000, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4c4fSU6MNbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with AdaDelta')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4yaj9hDLqSM",
        "colab_type": "text"
      },
      "source": [
        "As you can see, for this specific problem, AdaDelta does not do a good job. However, if you set the maximum number of iterations to some number, e.g. 5000, for both AdaGrad and AdaDelta and compare the results, you will see that AdaDelta jumps faster into a zone close to the optimum.\n",
        "\n",
        "The problems on the convergence in AdaDelta has been reported in the literature (e.g. Section 4.4 of this [book](https://books.google.de/books?id=IbnEDwAAQBAJ&pg=PA189&lpg=PA189&dq=adadelta+not+converging&source=bl&ots=f2i8liEovl&sig=ACfU3U2nzVAPCLLtC3Os_cxmHmh7acOBww&hl=en&sa=X&ved=2ahUKEwjl4YLJ7MTqAhWNs4sKHQUfCrQQ6AEwA3oECAgQAQ#v=onepage&q=adadelta%20not%20converging&f=false) and [this paper](https://openreview.net/pdf?id=ryQu7f-RZ)). And even the original paper make the comparison of AdaDelta saying:\n",
        "\n",
        "* *Setting the hyperparameters to $\\epsilon=1^{-6}$ and $\\rho=0.95$ we\n",
        "achieve 2.00% test set error compared to the 2.10% of Schaul\n",
        "et al. While this is nowhere near convergence it gives a sense\n",
        "of how quickly the algorithms can optimize the classification\n",
        "objective.*\n",
        "\n",
        "You might be asking why then AdaDelta is used, even if it fails to optimize our (relatively simple) Rosenbrock function. A posible answer is: in many machine learning applications, specially in Deep learning, being close to the optimum is enough for the task we want to accomplish. And if AdaDelta takes us relatively fast to this zone, then maybe is worth considering.\n",
        "\n",
        "Another interesting thing to note here is that even though you do not have a learning rate that has to be tunned, the hyperparameter $\\epsilon$ influences the behaviour of AdaDelta quite significantly for certain problems (e.g. ours here).\n",
        "\n",
        "<font color='blue'>Try different **combinations of the hyperparameters** $\\epsilon$ and $\\rho$ and see their influence in convergence. Also, **compare AdaGrad and AdaDelta** in terms of how far one goes towards the optimum in a fixed number of iterations. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48plrpuJ2Wts",
        "colab_type": "text"
      },
      "source": [
        "# **8. RMSProp**\n",
        "\n",
        "RMSProp was proposed by Geoffrey Hinton in his [Coursera lecture](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) and it is pretty much the same as the idea 1 presented at the [AdaDelta paper](https://arxiv.org/pdf/1212.5701.pdf). This idea is: adapt the learning rate using the gradiet information of the previous $w$ steps by computing the average exponential decay. Hence, RMSprop update rule is:\n",
        "\n",
        "$x^{(k)} = x^{(k)} - \\alpha \\frac{\\nabla f(x^{(k)})}{V^{(k)}}$\n",
        "\n",
        "where \n",
        "\n",
        "$V^{(k)} = \\sqrt{\\rho V^{(k-1)} + (1-\\rho) \\left(\\nabla f(x^{(k)})\\right)^2 + \\epsilon}$\n",
        "\n",
        "For the sake of completeness let's also implement it in such form.\n",
        "\n",
        "<font color='blue'>Code the **accumulation of squared gradients** of RMSProp.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0OycGIaMggi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################\n",
        "# --- RMSProp --- #\n",
        "####################\n",
        "\n",
        "def rmsprop(f, x0, grad_f, lr=0.001, rho=0.95, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    RMSProp optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        rho      : Exponential decay parameter\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    V = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#\n",
        "        V       =           # exponential decay average on gradients\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#\n",
        "        x       = x - lr*grad_i/V                                       # compute step\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using RMSProp \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dex0AysDMmTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- RMSProp --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = rmsprop(Rosenbrock_f, x0, central_finite_diff5, rho=0.9, eps=1e-8, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCKz5yfQNI8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with RMSProp')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQbnc_yQNEBa",
        "colab_type": "text"
      },
      "source": [
        "Well, this is a surprise, the algorithm converged to the optimum if we use the RMSProp form! \n",
        "\n",
        "Similar result was encounter in the [book](https://books.google.de/books?id=IbnEDwAAQBAJ&pg=PA189&lpg=PA189&dq=adadelta+not+converging&source=bl&ots=f2i8liEovl&sig=ACfU3U2nzVAPCLLtC3Os_cxmHmh7acOBww&hl=en&sa=X&ved=2ahUKEwjl4YLJ7MTqAhWNs4sKHQUfCrQQ6AEwA3oECAgQAQ#v=onepage&q=rmsprop&f=false) we mentioned aboved. The reason is that the accumulation of the parameters update ($E^{(k-1)}$ in AdaDelta) can act as an accelerator term at the fist iterations. However, when approximating the optimum, this same \"kind of momentum\" prevents the algorithm from convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Nt4ZaR2bpx",
        "colab_type": "text"
      },
      "source": [
        "# **9. Adam**\n",
        "\n",
        "Adam combines the nice key property of the momentum methods with the adaptive learning rate methods. In addition to keep the accumulation term of the squared gradients $V^{(k)}$, Adam also has an accumulation term for past gradients (like the momentum methods).\n",
        "\n",
        "The strategy of Adam is to calculate two moments for the gradients:\n",
        "\n",
        "*   First moment (mean): \n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ m^{(k)}=\\beta_1 m^{(k-1)} + (1-\\beta_1) \\nabla f(x^{(k)})$\n",
        "\n",
        "*   Second moment (uncentered variance):\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ V^{(k)}= \\beta_2 V^{(k-1)} + (1-\\beta_2) \\left( \\nabla f(x^{(k)}) \\right)^2$\n",
        "\n",
        "where $\\beta_1$ and $\\beta_2$ are exponential decay rates. Recommended values for $\\beta_1$, $\\beta_2$ and $\\epsilon$ are 0.9, 0.999 and $10^{-8}$ respectively.\n",
        "\n",
        "However, the [authors noted](https://arxiv.org/pdf/1412.6980.pdf) that during the first iterations the method is biased towards zero. Terefore, they used bias-corrected moments defined as:\n",
        "\n",
        "*   First bias-corrected moment (mean): \n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ \\hat{m}^{(k)}=\\frac{m^{(k)}}{1- \\beta_1^{k}}$\n",
        "\n",
        "*   Second moment (uncentered variance):\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ \\hat{V}^{(k)}= \\frac{V^{(k)}}{1- \\beta_2^{k}}$\n",
        "\n",
        "Note the terms $\\beta_1^{k}$ and $\\beta_2^{k}$ are the beta values to the power of the iteration number. Therefore, the update rule for Adam is:\n",
        "\n",
        "$x^{(k+1)} = x^{(k)} - \\alpha \\frac{\\hat{m}^{(k)}}{\\sqrt{\\hat{V}^{(k)}} + \\epsilon}$\n",
        "\n",
        "<font color='blue'>Implement the **moments equations** and the **update rule** of Adam.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnQ1Err1N5n7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################\n",
        "# --- Adam --- #\n",
        "################\n",
        "\n",
        "def adam(f, x0, grad_f, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Adam optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta_1   : Exponential decay parameter 1\n",
        "        beta_2   : Exponential decay parameter 2\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    m = 0;  V = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "        m       =                    # Moment 1\n",
        "        V       =                    # Moment 2\n",
        "        m_hat   =                    # Biased-corrected moment 1 \n",
        "        V_hat   =                    # Biased-corrected moment 2\n",
        "        x       =                    # compute step\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Adam \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ETEwLqQN95v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Adam --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = adam(Rosenbrock_f, x0, central_finite_diff5, lr=0.05, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3za4iNeAOD25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with Adam')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYSbDpor27Nf",
        "colab_type": "text"
      },
      "source": [
        "# **10. Newton's method**\n",
        "\n",
        "This optimization method calculates the inverse of the Hessian matrix to obtain faster convergence than the first-order gradient descent methods.\n",
        "\n",
        "The update rule of the Newton's method is:\n",
        "\n",
        "$ x^{(k+1)} = x^{(k)} - \\left(\\nabla^2 f(x^{(k)}) \\right)^{-1}\\nabla f(x^{(k)}) $\n",
        "\n",
        "The second term from the right hand side of the above equation is then the Newton step, and it requires, from the inverse of the Hessian, for the Hessian matrix to be positive definite. However, for some non-convex problems the Hessian might not be invertible.\n",
        "\n",
        "Similarly as what we did for approximating the gradient using central finite differences, we can approximate the Hessian. For the details and formulas of the Hessian approximation look [here](https://en.wikipedia.org/wiki/Finite_difference)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpLbclJtOUSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################################\n",
        "# --- Central second order finite differences --- #\n",
        "###################################################\n",
        "\n",
        "def Second_diff_fxx(f, x):\n",
        "    '''\n",
        "      Central finite differences approximation of Hessian\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the Hessian\n",
        "      OUTPUTS:\n",
        "          Hxx: Approximation of the Hessian of f at x \n",
        "      '''\n",
        "\n",
        "    dim   = np.shape(x)[1]\n",
        "    # Step-size is taken as the square root of the machine precision\n",
        "    eps  = np.sqrt(np.finfo(float).eps)\n",
        "    Hxx   = np.zeros((dim,dim))\n",
        "    \n",
        "    for j in range(dim):\n",
        "        # compute Fxx (diagonal elements)\n",
        "        x_d_f       = np.copy(x)             # forward step\n",
        "        x_d_b       = np.copy(x)             # backward step\n",
        "        x_d_f[0,j]  = x_d_f[0,j] + eps\n",
        "        x_d_b[0,j]  = x_d_b[0,j] - eps\n",
        "        Hxx[j,j]    = (f(x_d_f) -2*f(x) + f(x_d_b))/eps**2\n",
        "\n",
        "        for i in range(j+1,dim):\n",
        "            # compute Fxy (off-diagonal elements)\n",
        "            # Fxy\n",
        "            x_d_fxfy    = np.copy(x_d_f)\n",
        "            x_d_fxfy[0,i] = x_d_fxfy[0,i] + eps\n",
        "            x_d_fxby    = np.copy(x_d_f)\n",
        "            x_d_fxby[0,i] = x_d_fxby[0,i] - eps\n",
        "            x_d_bxfy    = np.copy(x_d_b)\n",
        "            x_d_bxfy[0,i] = x_d_bxfy[0,i] + eps\n",
        "            x_d_bxby    = np.copy(x_d_b)\n",
        "            x_d_bxby[0,i] = x_d_bxby[0,i] - eps\n",
        "            Hxx[j,i]    = (f(x_d_fxfy) - f(x_d_fxby) - f(x_d_bxfy) + f(x_d_bxby))/(4*eps**2)\n",
        "            Hxx[i,j]    = Hxx[j,i]\n",
        "\n",
        "    return Hxx\n",
        "\n",
        "\n",
        "x1       = np.array([2.,2.]).reshape(1,-1)\n",
        "Hxx = Second_diff_fxx(Rosenbrock_f, x1)\n",
        "print('Hxx = ',Hxx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Dmi0IgO0-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################\n",
        "# --- Newton's Method --- #\n",
        "###########################\n",
        "\n",
        "def newton(f, x0, grad_f, H_f, max_iter=1e3, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Newton's method\n",
        "\n",
        "    Note: the Hessian can become ill-conditioned\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        H_f      : Hessian function\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        traj     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    n      = np.shape(x0)[0]\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # trajectory\n",
        "    if traj == True:\n",
        "            x_list = []\n",
        "            f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:       \n",
        "        grad_i  = grad_f(f,x)                         # compute gradient\n",
        "        Hxx     = H_f(f,x)                            # compute Hessian\n",
        "        x       = x - (np.linalg.inv(Hxx)@grad_i.T).T # update              \n",
        "        iter_i += 1\n",
        "        \n",
        "        # trajectory\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "    \n",
        "    print(\" Optimization using Newton's method \\n\")\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "\n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list\n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3V7J59oO5Sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Newton's method --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = newton(Rosenbrock_f, x0, central_finite_diff5, Second_diff_fxx, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btYaAciHO9Ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title(\"Rosenbrock with Newton's method\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb3QmNBnPAb_",
        "colab_type": "text"
      },
      "source": [
        "Of course, using the Hessian information accelerates the optimization significantly! But, this comes at the expense of not only obtaining the Hessian, but also inverting it! This is the reason why second-order methods are not used in practice with large datasets. But, if the dataset is relatively small, go ahead and use second-order methods (whenever you have access to the second derivatives, of course)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NITxr1Ex3B67",
        "colab_type": "text"
      },
      "source": [
        "# **11. Constrained Newton's method**\n",
        "\n",
        "If we recall the Newton's method, the step (called Newton step) that we take at each iteration is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta x = - \\left(\\nabla^2 f(x^{(k)}) \\right)^{-1}\\nabla f(x^{(k)})\n",
        "\\end{equation}\n",
        "\n",
        "A nice interpretation of the Newtonâ€™s step is to set\n",
        "it by minimizing the second-order approximation of $f$ at $x$.\n",
        "In case of (equality) constrained problems, the Newton's method is slightly modified in two main aspects:\n",
        "\n",
        "* The initial point must be feasible.\n",
        "* The definition of Newton step is modified to make sure that the newton step is a feasible direction of the problem.\n",
        "\n",
        "To derive the Constrained Newton's method the objective function in the original problem:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x)\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x) = 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "is approximated by the second-order Taylor expansion:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{\\Delta x \\in X} \\quad & \\hat{f}(x + \\Delta x) = f(x) + \\nabla f(x)^T \\Delta x + \\frac{1}{2} \\Delta x ^T \\nabla^2 f(x) \\Delta x\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x + \\Delta x) = 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The Newton step $\\Delta x$ now can be obtained by solving the following system of equations: \n",
        "\n",
        "\n",
        "\\begin{gather}\n",
        " \\begin{bmatrix} \n",
        " \\nabla^2 f(x) & \\nabla h(x)^T \\\\ \n",
        " \\nabla h(x) & 0 \n",
        " \\end{bmatrix}\n",
        " \\begin{bmatrix} \n",
        " \\Delta x \\\\ \n",
        " \\lambda \n",
        " \\end{bmatrix}\n",
        " =\n",
        " -\n",
        " \\begin{bmatrix} \n",
        " \\nabla f(x) \\\\ \n",
        " h(x) \n",
        " \\end{bmatrix}\n",
        "\\end{gather}\n",
        "\n",
        "The stopping criterion is regularly defined by the Newton decrement as follows:\n",
        "\n",
        "\\begin{equation}\n",
        " \\frac{\\Delta x^T \\nabla^2 f(x) \\Delta x}{2} \\leq \\epsilon\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptglsOx_Uui7",
        "colab_type": "text"
      },
      "source": [
        "In this example, we are going to work with the following simple problem:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x_1, x_2 \\in X} \\quad & x_1^2 + 5x_2^2\\\\\n",
        "\\textrm{s.t.} \\quad & -x_1-x_2-2 = 0\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "Also, we are going to use the exact gradient and Hessian.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpG_QQ_F0vlD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Obj(x):\n",
        "  '''\n",
        "  Objective function\n",
        "  '''\n",
        "  return x[:,0]**2 + 5*x[:,1]**2\n",
        "\n",
        "def gradient(x):\n",
        "  '''\n",
        "  Gradient of objective function\n",
        "  '''\n",
        "  g1 = 2*x[:,0]\n",
        "  g2 = 10*x[:,1]\n",
        "  return  np.array([g1,g2])\n",
        "\n",
        "exact_hess = np.array([[2,0],[0, 10]])  # Exact Hessian of objective function\n",
        "\n",
        "def Const(x):\n",
        "  '''\n",
        "  Equality constrained\n",
        "  '''\n",
        "  return -x[:,0] - x[:,1] - 2\n",
        "\n",
        "J_constraint = np.array([-1, -1]).reshape(1,-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D965MBEhVsy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot \n",
        "x_1 = np.linspace(-4,4)\n",
        "x_2 = np.linspace(-2,2)\n",
        "x = np.vstack((x_1,x_2)).reshape(-1,2)\n",
        "X, Y = np.meshgrid(x_1, x_2)\n",
        "Z = Obj(np.append(X.reshape(-1,1), Y.reshape(-1,1), axis=1))\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "const_val = Const(x)\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2xzQtZxV0Uc",
        "colab_type": "text"
      },
      "source": [
        "First, let's solve it with a Scipy solver just for comparison and to make sure we obtain the same. Note we are using a different algorithm (SLSQP), since constrained optimization using Scipy is restricted to only a few methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfcQSjdL3_Hg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Scipy SLSQP\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def obj_f(x):\n",
        "  ''' Re-write objective function in the scipy form'''\n",
        "  return x[0]**2 + 5*x[1]**2\n",
        "\n",
        "initial = np.array([-2, 0])\n",
        "cons = ({'type': 'eq', 'fun': lambda x:  -x[0] -x[1] - 2})\n",
        "\n",
        "res = minimize(obj_f, initial, method='SLSQP', tol=1e-15, constraints=cons)\n",
        "print(res.fun)\n",
        "print(res.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3cftSZ8WrC2",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Implement the **constrained Newton step**.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQKdIvp55MjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################\n",
        "# --- Constrained Newton step --- #\n",
        "###################################\n",
        "\n",
        "def Const_Newton_step(H, J, g, h):\n",
        "  '''\n",
        "  Determine the constrained Newton step\n",
        "\n",
        "    INPUTS:\n",
        "      H: Hessian matrix of the objective function at x\n",
        "      J: Jacobian of constraints at x\n",
        "      g: Gradient of objective function at x\n",
        "      h: Constraints value at x\n",
        "        \n",
        "    OUTPUTS:\n",
        "      step: Constrained Newton step\n",
        "      lambda: Lagrange multipliers\n",
        "  '''\n",
        "  dim = H.shape[0]\n",
        "  num_const = J.shape[0]\n",
        "  dim_lag = dim + num_const\n",
        "  #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "  A                 =    # Create matrix of zeros\n",
        "  A[:dim,:dim]      =    # Substitute the Hessian of objective function\n",
        "  A[dim:,:dim][0]   =    # Substitute the Jacobian of constraints\n",
        "  A[:dim,dim:][:,0] =    # Substitute the Jacobian of constraints\n",
        "  B     =                # Append the gradient of objective function and constraint values \n",
        "  sol   =                # Solve the matrix system\n",
        "  #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#\n",
        "  step  = sol[:dim]\n",
        "  lamda = sol[dim:]\n",
        "\n",
        "  return step, lamda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt5VaYXDYafN",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Code the steps of the **constrained Newton's method**.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_5gDdaV0UEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################\n",
        "# --- Constrained Newton's Method --- #\n",
        "#######################################\n",
        "\n",
        "def const_newton(f, const, x0, grad_f, H_f, max_iter=1e3, tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Constrained Newton's method\n",
        "\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        const    : Constraint\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        H_f      : Hessian function\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        traj     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    decr   = 10\n",
        "    \n",
        "    # trajectory\n",
        "    if traj == True:\n",
        "            x_list = []\n",
        "            f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while decr/2 > tol and iter_i < max_iter:                            \n",
        "        \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "        H       =                # compute Hessian of Obj function\n",
        "        J       =                # compute Jacobian of constraint\n",
        "        g       =                # compute gradient of Obj function\n",
        "        h       =                # compute constraint value                     \n",
        "        delta_x, lamb =          # compute step and lambda\n",
        "        x       =                # update x\n",
        "        decr    =                # decrement    \n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#        \n",
        "        \n",
        "        iter_i += 1\n",
        "        \n",
        "        # trajectory\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "    \n",
        "    print(\" Optimization using constrained Newton's method \\n\")\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "\n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list\n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGXUPLX0wBU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Constrained Newton's method --- #\n",
        "x0 = np.array([3,1.5]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = const_newton(Obj, Const, x0, gradient, Second_diff_fxx, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUgJhLKUIew4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot solution\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.plot(x0[:,0], x0[:,1], 'ro', label='initial point')\n",
        "plt.plot(x_list[1][0], x_list[1][1], 'r*', label='Optimum', ms=10)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMub0N973Nh6",
        "colab_type": "text"
      },
      "source": [
        "# **12. Barrier method (interior-point methods)**\n",
        "\n",
        "With the Constrained Newton's method we accomplished to solve optimization problem with equality constraints. With interior-point methods we can solve convex problems that include inequality constraints. Interior-point methods solve this type of problems by solving a sequence of equality constrained problems using Newton's method. The barrier method is one of these interior-point algorithms, and is the one we are going to focus here.\n",
        "\n",
        "The goal is to approximately formulate the inequality constrained problem \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x)\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x) = 0, i= 1, ..., m\\\\\n",
        "\\quad & g_j(x) \\leq 0, j=1,...,n\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "as an equality constrained problem to which Newtonâ€™s method can be applied. This is accomplished by making the inequality constraints $g_j(x)$ implicit in the objective function $f(x)$.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x) + \\sum_{j=1}^{n}I_-(g_j(x))\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x) = 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "where $I_-$ is the indicator function for the nonpositive reals:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "    I_-(u)= \n",
        "\\begin{cases}\n",
        "    0, &  u \\leq 0\\\\\n",
        "    \\infty,    & u > 0\n",
        "\\end{cases}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The problem with this formulation is that, even when we accomplished to make the inequality constraints implicit in the objective function, this objective function is not (in general) differentiable, so Newton's method cannot be applied. The basic idea of the barrier method is to approximate this indicator function as:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\hat{I}_-(u) = - \\frac{1}{t} \\log (-u) \n",
        "\\end{equation}\n",
        "\n",
        "Therefore, the approximated problem that is going to be solved is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x) + \\sum_{j=1}^{n}- \\frac{1}{t} \\log (-g_j(x))\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x) = 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "where the function $\\phi(x) = - \\sum_{j=1}^{n} \\log (-g_j(x))$ is called the *logarithmic barrier* for the problem.\n",
        "\n",
        "The accuracy of this approximation improves as the parameter $t$ increases. However, when the parameter $t$ is large, the objective function\n",
        "is difficult to minimize by Newtonâ€™s method, since its Hessian varies rapidly near the boundary of the feasible set. This problem can be overcomed by solving a sequence of problems of the same form, increasing the parameter $t$ (and therefore the accuracy of the approximation) at each step, and starting each Newton minimization at the solution of the problem for the previous value of t.\n",
        "\n",
        "The gradient and the Hessian of the logarithmic barrier function are given by:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\nabla \\phi (x) = \\sum_{j=1}^{n} - \\frac{1}{g_j(x)} \\nabla g_j(x)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\nabla^2 \\phi (x) = \\sum_{j=1}^{n} \\frac{1}{g_j(x)^2} \\nabla g_j(x) \\nabla g_j(x)^T - \\sum_{j=1}^{n} \\frac{1}{g_j(x)} \\nabla^2 g_j(x)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "**Algorithm**\n",
        "\n",
        "1. Given a strictly feasible starting point $x \\in X$, $t := t^{(0)}$ $\\alpha > 0$, $\\epsilon >0$.\n",
        "2. Repeat until $n/t \\leq \\epsilon$\n",
        "3. $~~~~~~$ Solve approximated problem with $t:= t^{(k)}$ starting from $x^{k}$ using Newton's method\n",
        "2. $~~~~~~$ Update $x^{k+1} := x^{*(k)}$\n",
        "3. $~~~~~~$ Increase $t^{k+1} := \\alpha t^{(k)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rivDA_3ThIzj",
        "colab_type": "text"
      },
      "source": [
        "The example problem to solve is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x_1, x_2 \\in X} \\quad & x_1^2 + 5x_2^2\\\\\n",
        "\\textrm{s.t.} \\quad & -x_1-x_2-2 = 0\\\\\n",
        "& -x_1-x_2 \\leq 0\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "Here, we will use exact gradients and Hessians."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQur-ADLk_x2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Ineq_const(x):\n",
        "  '''\n",
        "  Inequality constraint\n",
        "  '''\n",
        "  return -x[:,0] + x[:,1]\n",
        "\n",
        "grad_ineq = np.array([-1, 1])           # Gradient of inequality constraint\n",
        "hess_ineq = np.array([[0,0], [0,0]])    # Hessian of inequality constraint\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUkyG7X9aa6w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot\n",
        "x = np.vstack((x_1,x_2)).reshape(-1,2)\n",
        "ineq_const_val = Ineq_const(x)\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.plot(x_1, x_1, 'b--',label='g(x)=0')\n",
        "plt.fill_between(x_1, x_1, np.repeat(4, len(x_1)), color='k', alpha=0.3)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPWQPRQ8bPLF",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Observe the impact of the parameter $t$ on the approximation of the objective function of the barrier method. Play, by increasing its value to see how this approximation changes.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQP2haywT8Si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################################\n",
        "# --- Approximation with log barrier --- #\n",
        "##########################################\n",
        "\n",
        "def Obj_barrier(x):\n",
        "  '''\n",
        "  Objective function with log barrier\n",
        "  '''\n",
        "  t = 0.1\n",
        "  phi = - np.log(-Ineq_const(x))\n",
        "  return x[:,0]**2 + (5*x[:,1])**2 + 1/t*phi\n",
        "\n",
        "# Plot approximation of log barrier\n",
        "Z2 = Obj_barrier(np.append(X.reshape(-1,1), Y.reshape(-1,1), axis=1))\n",
        "Z2 = Z2.reshape(X.shape)\n",
        "const_val = Const(x)\n",
        "x = np.vstack((x_1,x_2)).reshape(-1,2)\n",
        "ineq_const_val = Ineq_const(x)\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "plt.contour(x_1, x_2, Z2, colors='green', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.plot(x_1, x_1, 'b--',label='g(x)=0')\n",
        "plt.fill_between(x_1, x_1, np.repeat(4, len(x_1)), color='k', alpha=0.3)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7ghxMlVsjGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using Scipy SLSQP to see what to expect\n",
        "\n",
        "initial = np.array([-2, 0])\n",
        "cons = ({'type': 'eq', 'fun': lambda x:  -x[0] -x[1] - 2},\n",
        "        {'type': 'ineq', 'fun': lambda x:  x[0] -x[1]})\n",
        "\n",
        "res = minimize(obj_f, initial, method='SLSQP', tol=1e-15, constraints=cons)\n",
        "print(res.fun)\n",
        "print(res.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3blv4H1pfUfK",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Code the **Hessian and the gradient of the logarithmic barrier**.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Ma-ghfjBhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#######################################\n",
        "# --- Hessian logarithmic barrier --- #\n",
        "#######################################\n",
        "\n",
        "def hess_logbarrier(x):\n",
        "  '''\n",
        "  Hessian of the logarithmic barrier function\n",
        "  '''\n",
        "  val_ineq = Ineq_const(x)[0]         # Value of the inequality constraint at x\n",
        "  #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "  h1 =          # First term \n",
        "  h2 =          # Second term\n",
        "  #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#\n",
        "  return h1 + h2\n",
        "\n",
        "########################################\n",
        "# --- Gradient logarithmic barrier --- #\n",
        "########################################\n",
        "\n",
        "def grad_logbarrier(x):\n",
        "  '''\n",
        "  Gradient of the logarithmic barrier function\n",
        "  '''\n",
        "  val_ineq = Ineq_const(x)[0]         # Value of the inequality constraint at x\n",
        "  #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "  g1 =                                # Gradient of log barrier\n",
        "  #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#\n",
        "  return g1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltKDY7ecfo1K",
        "colab_type": "text"
      },
      "source": [
        "<font color='blue'>Implement the **barrier method steps**.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAkKWp7EHwND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##########################\n",
        "# --- Barrier method --- #\n",
        "##########################\n",
        "\n",
        "def barrier(f, eq_const, x0, grad_f, t, alpha=1.01, max_iter=1e4, tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Barrier method (interior-point methods)\n",
        "\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        eq_const : Equality constraint\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        t        : Barrier parameter\n",
        "        alpha    : Parameter to increase t\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        traj     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    n_iq   = 1             # number of inequality constraints\n",
        "    \n",
        "    # trajectory\n",
        "    if traj == True:\n",
        "            x_list = []\n",
        "            f_list = []            \n",
        "\n",
        "    # optimization loop\n",
        "    while n_iq/t >= tol*0.001 and iter_i < max_iter: \n",
        "      decr = 10\n",
        "      while decr/2 > tol and iter_i < max_iter:\n",
        "        \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "        H =                # Hessian of objective function\n",
        "        J =                # compute Jacobian of constraint\n",
        "        g =                # Gradient of objective function\n",
        "        h =                # compute (equality) constraint value\n",
        "        delta_x, la =      # compute step and lambda\n",
        "        x       =          # update x\n",
        "        decr    =          # decrement\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#\n",
        "        \n",
        "        iter_i += 1\n",
        "      t       = t * alpha                         # update t\n",
        "      \n",
        "      # trajectory\n",
        "      if traj == True:\n",
        "          x_list.append(x.flatten().tolist())\n",
        "          f_list.append(f(x))\n",
        "    \n",
        "    print(\" Optimization using barrier method \\n\")\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "\n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list\n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYra2hMcBxNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- Barrier method --- #\n",
        "x0 = np.array([3.5,1.5]).reshape(1,-1)\n",
        "t0 = 0.001\n",
        "\n",
        "xf, x_list, f_list = barrier(Obj, Const, x0, gradient, t0, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Wwch13EKQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot solution\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x0[:,0], x0[:,1], 'ro', label='initial point')\n",
        "plt.plot(x_list[-1][0], x_list[-1][1], 'r*', label='Optimum', ms=10)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.plot(x_1, x_1, 'b--',label='g(x)=0')\n",
        "plt.fill_between(x_1, x_1, np.repeat(4, len(x_1)), color='k', alpha=0.3)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}